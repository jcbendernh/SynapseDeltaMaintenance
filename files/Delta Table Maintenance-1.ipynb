{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Delta Table Maintenance Part 1 - Prep Notebook\r\n",
        "This notebook is used to query spark for the listing of Delta Tables so it can be passed onto a subsequent pipeline activity for Notebook maintenance.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Declare ADLS Gen2 value to pass to the DataFrame"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession \r\n",
        "from pyspark.sql.types import * \r\n",
        "\r\n",
        "# Primary storage info \r\n",
        "account_name = 'demosynapsestoragejb' # fill in your primary account name that is associated with the Synapse WOrkspace\r\n",
        "container_name = 'other' # fill in your container name, you need to manually create this in your ADLS Gen2 account.\r\n",
        "relative_path = 'maintenance/DeltaTablesList/' # fill in your relative folder path \r\n",
        "\r\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
        "print('Primary storage account path: ' + adls_path) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use the snippet below to request a listing of the Delta tables to be passed to the dataframe"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tablesDF = spark.sql(\"SHOW TABLES\")\r\n",
        "tablesDF.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filter out all temporary tables by tableName\r\n",
        "This is an extra step to filter the listing if you have temp tables.  They may be named differently than mine and your wuery may need to be adjusted."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtlDF = tablesDF.where(\"NOT(tableName like 'tmp%')\")\r\n",
        "dtlDF.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the Datafame to the ADLS Gen 2 folder.  \r\n",
        "The repartition command keeps everything as one file."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtlDF.repartition(1) \\\r\n",
        "    .write.csv(adls_path, mode = 'overwrite', header = 'true')\r\n",
        "    # .write.parquet(adls_path, mode = 'overwrite') "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}