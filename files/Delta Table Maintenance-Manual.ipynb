{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Delta Table Maintenance\r\n",
        "\r\n",
        "This Notebook is used to perform maintenance tasks on a single Delta Table. For more on this topic check out https://docs.delta.io/latest/delta-utility.html#remove-files-no-longer-referenced-by-a-delta-table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 1 - Check to see if the table Delta Table exists.\r\n",
        "If you see it in the listing, skip to step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "SHOW TABLES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 2  - Declare the values to pass on the other cells later in the notebook.\r\n",
        "This is the location of your delta table parquet files and the delta_log directory in your ADLS Gen2 account.  \r\n",
        "This step is only needed if you delta table did not show in the listing above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession \r\n",
        "from pyspark.sql.types import * \r\n",
        "from delta.tables import DeltaTable\r\n",
        "from notebookutils import mssparkutils\r\n",
        "\r\n",
        "# Primary storage info \r\n",
        "account_name = 'demosynapsesilver' # fill in your primary account name \r\n",
        "container_name = 'sales' # fill in your container name\r\n",
        "relative_path = 'customers/' # fill in your relative folder path \r\n",
        "\r\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
        "print('Primary storage account path: ' + adls_path) \r\n",
        "spark.conf.set(\"sql.adls_path\", adls_path) #Used for Spark SQL commands in later cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Shows the files in your ADLS Gen 2 account.  This verifies that the path above is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "files = mssparkutils.fs.ls(adls_path)\r\n",
        "for file in files:\r\n",
        "    print(file.path, file.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 3 - Declare our Delta Table values\r\n",
        "If your Delta Table Name was in the listing of step 1, add it below, replacing the SilverCustomers value.  Otherwise declare your Delta table as needed.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Delta Table Settings\r\n",
        "spark.conf.set(\"sql.delta_table\", 'SilverCustomers') #Delta Table Name\r\n",
        "spark.conf.set(\"sql.vacuum_hours\", 168)  # Vacuum setting for the Delta Table - 168 hours equals 1 week"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 4 - Create the Delta table if it did not show in the listing above\r\n",
        "Skip this step if your Delta Table Name was in the listing of step 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql \r\n",
        "\r\n",
        "CREATE TABLE IF NOT EXISTS ${sql.delta_table}    \r\n",
        "USING DELTA\r\n",
        "LOCATION '${sql.adls_path}';"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 5 - Get a the properties of the Delta Table\r\n",
        "This is a verification step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "DESCRIBE DETAIL ${sql.delta_table}\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 6 - Optimize Table Command\r\n",
        "You can add more options to the command.  For more on this topic check out https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "OPTIMIZE ${sql.delta_table};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 7 -  Vacuum table\r\n",
        "You can add more options to command.  For more on this topic check out https://learn.microsoft.com/en-us/azure/databricks/delta/vacuum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "VACUUM ${sql.delta_table} RETAIN ${sql.vacuum_hours} HOURS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Step 8 - Shows the files in Storage after Vacuum Command\r\n",
        "This is a verification step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      },
      "source": [
        "files = mssparkutils.fs.ls(adls_path)\r\n",
        "for file in files:\r\n",
        "    print(file.path, file.size)"
      ]
    }
  ]
}